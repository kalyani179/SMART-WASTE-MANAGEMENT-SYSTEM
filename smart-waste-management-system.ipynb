{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalyani179/SMART-WASTE-MANAGEMENT-SYSTEM/blob/main/smart-waste-management-system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "gbQk8VMrT40L",
        "outputId": "5b88cab9-3ad0-4e67-db1f-6f2a743fc78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory trashnet-master/data/resized_data/dataset-resized/plastic does not exist.\n",
            "Directory trashnet-master/data/resized_data/dataset-resized/paper does not exist.\n",
            "Directory trashnet-master/data/resized_data/dataset-resized/cardboard does not exist.\n",
            "Directory trashnet-master/data/resized_data/dataset-resized/trash does not exist.\n",
            "Directory trashnet-master/data/resized_data/dataset-resized/glass does not exist.\n",
            "Directory trashnet-master/data/resized_data/dataset-resized/metal does not exist.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No images were loaded. Please check the dataset and directory structure.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d03bb599d893>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Check if images were loaded successfully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No images were loaded. Please check the dataset and directory structure.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Step 3: Define and train the CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No images were loaded. Please check the dataset and directory structure."
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import layers, models\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import requests\n",
        "\n",
        "# Step 1: Download the TrashNet Dataset\n",
        "def download_trashnet_dataset():\n",
        "    dataset_url = 'https://github.com/garythung/trashnet/archive/refs/heads/master.zip'\n",
        "    response = requests.get(dataset_url)\n",
        "    zip_file_path = 'trashnet.zip'\n",
        "\n",
        "    # Save the zip file\n",
        "    with open(zip_file_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "\n",
        "    return 'trashnet-master/data/resized_data/dataset-resized'\n",
        "\n",
        "# Step 2: Load the dataset and prepare data\n",
        "dataset_path = download_trashnet_dataset()\n",
        "\n",
        "# Set image size and categories\n",
        "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "categories = ['plastic', 'paper', 'cardboard', 'trash', 'glass', 'metal']\n",
        "\n",
        "# Prepare image data for training\n",
        "def load_images(image_dir):\n",
        "    images, labels = [], []\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(image_dir, category)\n",
        "        if not os.path.exists(category_dir):\n",
        "            print(f\"Directory {category_dir} does not exist.\")\n",
        "            continue\n",
        "        print(f\"Loading images from {category_dir}...\")\n",
        "        for img_file in os.listdir(category_dir):\n",
        "            img_path = os.path.join(category_dir, img_file)\n",
        "            try:\n",
        "                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "                img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "                images.append(img_array)\n",
        "                labels.append(categories.index(category))  # Label as index of category\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load the images from the extracted folder\n",
        "images, labels = load_images(dataset_path)\n",
        "\n",
        "# Check if images were loaded successfully\n",
        "if len(images) == 0:\n",
        "    raise ValueError(\"No images were loaded. Please check the dataset and directory structure.\")\n",
        "\n",
        "# Step 3: Define and train the CNN model\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(len(categories), activation='softmax')  # Output layer for categories\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = create_model()\n",
        "model.fit(images, labels, epochs=10, batch_size=BATCH_SIZE, validation_split=0.2)\n",
        "\n",
        "# Step 4: Define recycling methods (example data)\n",
        "recycling_methods = {\n",
        "    'plastic': {\n",
        "        'Mechanical Recycling': {'Efficiency': 80, 'Cost': 0.25, 'Energy': 1.5, 'Environmental Impact': 3},\n",
        "        'Chemical Recycling': {'Efficiency': 85, 'Cost': 0.30, 'Energy': 2, 'Environmental Impact': 4},\n",
        "    },\n",
        "    'paper': {\n",
        "        'Composting': {'Efficiency': 60, 'Cost': 0.05, 'Energy': 0.2, 'Environmental Impact': 2},\n",
        "    },\n",
        "    'cardboard': {\n",
        "        'Recycling': {'Efficiency': 75, 'Cost': 0.10, 'Energy': 0.5, 'Environmental Impact': 3},\n",
        "    },\n",
        "    'glass': {\n",
        "        'Recycling': {'Efficiency': 90, 'Cost': 0.15, 'Energy': 1.0, 'Environmental Impact': 2},\n",
        "    },\n",
        "    'metal': {\n",
        "        'Mechanical Recycling': {'Efficiency': 90, 'Cost': 0.15, 'Energy': 1.5, 'Environmental Impact': 1},\n",
        "    },\n",
        "    'trash': {\n",
        "        'Landfill': {'Efficiency': 50, 'Cost': 0.05, 'Energy': 0.0, 'Environmental Impact': 4},\n",
        "    },\n",
        "}\n",
        "\n",
        "# Step 5: Recommend the best recycling method\n",
        "def recommend_recycling(waste_type):\n",
        "    methods = recycling_methods.get(waste_type, {})\n",
        "    if not methods:\n",
        "        return \"No recycling methods available for this waste type.\"\n",
        "\n",
        "    scores = {}\n",
        "    for method, attributes in methods.items():\n",
        "        score = (attributes['Efficiency'] * 0.4 +\n",
        "                 (1/attributes['Cost']) * 0.2 +  # Inverse cost for scoring\n",
        "                 (1/attributes['Energy']) * 0.2 +  # Inverse energy for scoring\n",
        "                 attributes['Environmental Impact'] * 0.2)\n",
        "        scores[method] = score\n",
        "\n",
        "    best_method = max(scores, key=scores.get)\n",
        "    return best_method, scores[best_method]\n",
        "\n",
        "# Example usage\n",
        "uploaded = files.upload()  # Upload an image for classification\n",
        "uploaded_image_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load the uploaded image and predict waste type\n",
        "uploaded_img = load_img(uploaded_image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "uploaded_img_array = img_to_array(uploaded_img) / 255.0\n",
        "uploaded_img_array = np.expand_dims(uploaded_img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "predicted_class = model.predict(uploaded_img_array)\n",
        "predicted_label = categories[np.argmax(predicted_class)]\n",
        "\n",
        "# Recommend recycling method based on predicted waste type\n",
        "recommended_method, score = recommend_recycling(predicted_label)\n",
        "\n",
        "print(f\"Predicted Waste Type: {predicted_label}\")\n",
        "print(f\"Recommended Recycling Method: {recommended_method} (Score: {score})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P4W9C8qUkkpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the contents of the extracted dataset\n",
        "def check_extracted_contents(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        print(f\"Found directory: {root}\")\n",
        "        for d in dirs:\n",
        "            print(f\"  Subdirectory: {d}\")\n",
        "        for f in files:\n",
        "            print(f\"  File: {f}\")\n",
        "\n",
        "check_extracted_contents('trashnet-master')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5UN5pDCWmM3",
        "outputId": "91054a91-ea10-41d5-b4f7-77a1547d85cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found directory: trashnet-master\n",
            "  Subdirectory: data\n",
            "  File: weight-init.lua\n",
            "  File: LICENSE\n",
            "  File: train.lua\n",
            "  File: README.md\n",
            "  File: DataLoader.lua\n",
            "  File: utils.lua\n",
            "  File: plot.lua\n",
            "  File: test.lua\n",
            "  File: model.lua\n",
            "  File: .gitignore\n",
            "  File: shuffle.lua\n",
            "Found directory: trashnet-master/data\n",
            "  File: one-indexed-files-notrash_val.txt\n",
            "  File: dataset-resized.zip\n",
            "  File: one-indexed-files-notrash_test.txt\n",
            "  File: resize.py\n",
            "  File: one-indexed-files-notrash_train.txt\n",
            "  File: one-indexed-files.txt\n",
            "  File: constants.py\n",
            "  File: zero-indexed-files.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define paths and parameters\n",
        "dataset_path = \"trashnet-master/data/resized_data/dataset-resized\"\n",
        "cache_path = \"resized_cache\"\n",
        "IMG_HEIGHT, IMG_WIDTH = 256, 256\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "categories = [\"cardboard\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
        "\n",
        "# Create cache directory if it doesn't exist\n",
        "os.makedirs(cache_path, exist_ok=True)\n",
        "\n",
        "# Load and cache images\n",
        "def load_and_cache_images(dataset_path, cache_path):\n",
        "    images, labels = [], []\n",
        "    for category in categories:\n",
        "        category_dir = os.path.join(dataset_path, category)\n",
        "        cache_category_dir = os.path.join(cache_path, category)\n",
        "        os.makedirs(cache_category_dir, exist_ok=True)\n",
        "\n",
        "        for img_file in os.listdir(category_dir):\n",
        "            cache_img_path = os.path.join(cache_category_dir, img_file)\n",
        "            if os.path.exists(cache_img_path):\n",
        "                # Load cached image\n",
        "                img = load_img(cache_img_path)\n",
        "            else:\n",
        "                # Load, resize and save to cache\n",
        "                img_path = os.path.join(category_dir, img_file)\n",
        "                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "                save_img(cache_img_path, img)\n",
        "\n",
        "            img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
        "            images.append(img_array)\n",
        "            labels.append(categories.index(category))\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# First load and cache images\n",
        "images, labels = load_and_cache_images(dataset_path, cache_path)\n",
        "\n",
        "# Validate data loading\n",
        "if len(images) == 0:\n",
        "    raise ValueError(\"No images were loaded. Please check the dataset and directory structure.\")\n",
        "print(f\"Loaded {len(images)} images.\")\n",
        "\n",
        "# Split data into training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ui_sYMXtkmLS",
        "outputId": "49d0feb4-5e30-4f32-c1d6-999caf5a19b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'trashnet-master/data/resized_data/dataset-resized/cardboard'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1e90af2994ca>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# First load and cache images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_cache_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Validate data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1e90af2994ca>\u001b[0m in \u001b[0;36mload_and_cache_images\u001b[0;34m(dataset_path, cache_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_category_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mcache_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_category_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trashnet-master/data/resized_data/dataset-resized/cardboard'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(len(categories), activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize model\n",
        "model = create_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17q1PWnsk0Ie",
        "outputId": "aaacf46c-01aa-41d8-f16a-6ae535bbef7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "DDy9KuDak6PK",
        "outputId": "1ef83190-6a3c-4394-9568-05fee09eb9c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f916bf708b35>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    }
  ]
}